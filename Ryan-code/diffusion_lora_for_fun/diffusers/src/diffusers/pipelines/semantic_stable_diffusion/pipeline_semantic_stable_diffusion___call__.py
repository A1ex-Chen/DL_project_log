@torch.no_grad()
def __call__(self, prompt: Union[str, List[str]], height: Optional[int]=
    None, width: Optional[int]=None, num_inference_steps: int=50,
    guidance_scale: float=7.5, negative_prompt: Optional[Union[str, List[
    str]]]=None, num_images_per_prompt: int=1, eta: float=0.0, generator:
    Optional[Union[torch.Generator, List[torch.Generator]]]=None, latents:
    Optional[torch.Tensor]=None, output_type: Optional[str]='pil',
    return_dict: bool=True, callback: Optional[Callable[[int, int, torch.
    Tensor], None]]=None, callback_steps: int=1, editing_prompt: Optional[
    Union[str, List[str]]]=None, editing_prompt_embeddings: Optional[torch.
    Tensor]=None, reverse_editing_direction: Optional[Union[bool, List[bool
    ]]]=False, edit_guidance_scale: Optional[Union[float, List[float]]]=5,
    edit_warmup_steps: Optional[Union[int, List[int]]]=10,
    edit_cooldown_steps: Optional[Union[int, List[int]]]=None,
    edit_threshold: Optional[Union[float, List[float]]]=0.9,
    edit_momentum_scale: Optional[float]=0.1, edit_mom_beta: Optional[float
    ]=0.4, edit_weights: Optional[List[float]]=None, sem_guidance: Optional
    [List[torch.Tensor]]=None):
    """
        The call function to the pipeline for generation.

        Args:
            prompt (`str` or `List[str]`):
                The prompt or prompts to guide image generation.
            height (`int`, *optional*, defaults to `self.unet.config.sample_size * self.vae_scale_factor`):
                The height in pixels of the generated image.
            width (`int`, *optional*, defaults to `self.unet.config.sample_size * self.vae_scale_factor`):
                The width in pixels of the generated image.
            num_inference_steps (`int`, *optional*, defaults to 50):
                The number of denoising steps. More denoising steps usually lead to a higher quality image at the
                expense of slower inference.
            guidance_scale (`float`, *optional*, defaults to 7.5):
                A higher guidance scale value encourages the model to generate images closely linked to the text
                `prompt` at the expense of lower image quality. Guidance scale is enabled when `guidance_scale > 1`.
            negative_prompt (`str` or `List[str]`, *optional*):
                The prompt or prompts to guide what to not include in image generation. If not defined, you need to
                pass `negative_prompt_embeds` instead. Ignored when not using guidance (`guidance_scale < 1`).
            num_images_per_prompt (`int`, *optional*, defaults to 1):
                The number of images to generate per prompt.
            eta (`float`, *optional*, defaults to 0.0):
                Corresponds to parameter eta (Î·) from the [DDIM](https://arxiv.org/abs/2010.02502) paper. Only applies
                to the [`~schedulers.DDIMScheduler`], and is ignored in other schedulers.
            generator (`torch.Generator` or `List[torch.Generator]`, *optional*):
                A [`torch.Generator`](https://pytorch.org/docs/stable/generated/torch.Generator.html) to make
                generation deterministic.
            latents (`torch.Tensor`, *optional*):
                Pre-generated noisy latents sampled from a Gaussian distribution, to be used as inputs for image
                generation. Can be used to tweak the same generation with different prompts. If not provided, a latents
                tensor is generated by sampling using the supplied random `generator`.
            output_type (`str`, *optional*, defaults to `"pil"`):
                The output format of the generated image. Choose between `PIL.Image` or `np.array`.
            return_dict (`bool`, *optional*, defaults to `True`):
                Whether or not to return a [`~pipelines.stable_diffusion.StableDiffusionPipelineOutput`] instead of a
                plain tuple.
            callback (`Callable`, *optional*):
                A function that calls every `callback_steps` steps during inference. The function is called with the
                following arguments: `callback(step: int, timestep: int, latents: torch.Tensor)`.
            callback_steps (`int`, *optional*, defaults to 1):
                The frequency at which the `callback` function is called. If not specified, the callback is called at
                every step.
            editing_prompt (`str` or `List[str]`, *optional*):
                The prompt or prompts to use for semantic guidance. Semantic guidance is disabled by setting
                `editing_prompt = None`. Guidance direction of prompt should be specified via
                `reverse_editing_direction`.
            editing_prompt_embeddings (`torch.Tensor`, *optional*):
                Pre-computed embeddings to use for semantic guidance. Guidance direction of embedding should be
                specified via `reverse_editing_direction`.
            reverse_editing_direction (`bool` or `List[bool]`, *optional*, defaults to `False`):
                Whether the corresponding prompt in `editing_prompt` should be increased or decreased.
            edit_guidance_scale (`float` or `List[float]`, *optional*, defaults to 5):
                Guidance scale for semantic guidance. If provided as a list, values should correspond to
                `editing_prompt`.
            edit_warmup_steps (`float` or `List[float]`, *optional*, defaults to 10):
                Number of diffusion steps (for each prompt) for which semantic guidance is not applied. Momentum is
                calculated for those steps and applied once all warmup periods are over.
            edit_cooldown_steps (`float` or `List[float]`, *optional*, defaults to `None`):
                Number of diffusion steps (for each prompt) after which semantic guidance is longer applied.
            edit_threshold (`float` or `List[float]`, *optional*, defaults to 0.9):
                Threshold of semantic guidance.
            edit_momentum_scale (`float`, *optional*, defaults to 0.1):
                Scale of the momentum to be added to the semantic guidance at each diffusion step. If set to 0.0,
                momentum is disabled. Momentum is already built up during warmup (for diffusion steps smaller than
                `sld_warmup_steps`). Momentum is only added to latent guidance once all warmup periods are finished.
            edit_mom_beta (`float`, *optional*, defaults to 0.4):
                Defines how semantic guidance momentum builds up. `edit_mom_beta` indicates how much of the previous
                momentum is kept. Momentum is already built up during warmup (for diffusion steps smaller than
                `edit_warmup_steps`).
            edit_weights (`List[float]`, *optional*, defaults to `None`):
                Indicates how much each individual concept should influence the overall guidance. If no weights are
                provided all concepts are applied equally.
            sem_guidance (`List[torch.Tensor]`, *optional*):
                List of pre-generated guidance vectors to be applied at generation. Length of the list has to
                correspond to `num_inference_steps`.

        Examples:

        ```py
        >>> import torch
        >>> from diffusers import SemanticStableDiffusionPipeline

        >>> pipe = SemanticStableDiffusionPipeline.from_pretrained(
        ...     "runwayml/stable-diffusion-v1-5", torch_dtype=torch.float16
        ... )
        >>> pipe = pipe.to("cuda")

        >>> out = pipe(
        ...     prompt="a photo of the face of a woman",
        ...     num_images_per_prompt=1,
        ...     guidance_scale=7,
        ...     editing_prompt=[
        ...         "smiling, smile",  # Concepts to apply
        ...         "glasses, wearing glasses",
        ...         "curls, wavy hair, curly hair",
        ...         "beard, full beard, mustache",
        ...     ],
        ...     reverse_editing_direction=[
        ...         False,
        ...         False,
        ...         False,
        ...         False,
        ...     ],  # Direction of guidance i.e. increase all concepts
        ...     edit_warmup_steps=[10, 10, 10, 10],  # Warmup period for each concept
        ...     edit_guidance_scale=[4, 5, 5, 5.4],  # Guidance scale for each concept
        ...     edit_threshold=[
        ...         0.99,
        ...         0.975,
        ...         0.925,
        ...         0.96,
        ...     ],  # Threshold for each concept. Threshold equals the percentile of the latent space that will be discarded. I.e. threshold=0.99 uses 1% of the latent dimensions
        ...     edit_momentum_scale=0.3,  # Momentum scale that will be added to the latent guidance
        ...     edit_mom_beta=0.6,  # Momentum beta
        ...     edit_weights=[1, 1, 1, 1, 1],  # Weights of the individual concepts against each other
        ... )
        >>> image = out.images[0]
        ```

        Returns:
            [`~pipelines.semantic_stable_diffusion.SemanticStableDiffusionPipelineOutput`] or `tuple`:
                If `return_dict` is `True`,
                [`~pipelines.semantic_stable_diffusion.SemanticStableDiffusionPipelineOutput`] is returned, otherwise a
                `tuple` is returned where the first element is a list with the generated images and the second element
                is a list of `bool`s indicating whether the corresponding generated image contains "not-safe-for-work"
                (nsfw) content.
        """
    height = height or self.unet.config.sample_size * self.vae_scale_factor
    width = width or self.unet.config.sample_size * self.vae_scale_factor
    self.check_inputs(prompt, height, width, callback_steps)
    batch_size = 1 if isinstance(prompt, str) else len(prompt)
    if editing_prompt:
        enable_edit_guidance = True
        if isinstance(editing_prompt, str):
            editing_prompt = [editing_prompt]
        enabled_editing_prompts = len(editing_prompt)
    elif editing_prompt_embeddings is not None:
        enable_edit_guidance = True
        enabled_editing_prompts = editing_prompt_embeddings.shape[0]
    else:
        enabled_editing_prompts = 0
        enable_edit_guidance = False
    text_inputs = self.tokenizer(prompt, padding='max_length', max_length=
        self.tokenizer.model_max_length, return_tensors='pt')
    text_input_ids = text_inputs.input_ids
    if text_input_ids.shape[-1] > self.tokenizer.model_max_length:
        removed_text = self.tokenizer.batch_decode(text_input_ids[:, self.
            tokenizer.model_max_length:])
        logger.warning(
            f'The following part of your input was truncated because CLIP can only handle sequences up to {self.tokenizer.model_max_length} tokens: {removed_text}'
            )
        text_input_ids = text_input_ids[:, :self.tokenizer.model_max_length]
    text_embeddings = self.text_encoder(text_input_ids.to(self.device))[0]
    bs_embed, seq_len, _ = text_embeddings.shape
    text_embeddings = text_embeddings.repeat(1, num_images_per_prompt, 1)
    text_embeddings = text_embeddings.view(bs_embed * num_images_per_prompt,
        seq_len, -1)
    if enable_edit_guidance:
        if editing_prompt_embeddings is None:
            edit_concepts_input = self.tokenizer([x for item in
                editing_prompt for x in repeat(item, batch_size)], padding=
                'max_length', max_length=self.tokenizer.model_max_length,
                return_tensors='pt')
            edit_concepts_input_ids = edit_concepts_input.input_ids
            if edit_concepts_input_ids.shape[-1
                ] > self.tokenizer.model_max_length:
                removed_text = self.tokenizer.batch_decode(
                    edit_concepts_input_ids[:, self.tokenizer.
                    model_max_length:])
                logger.warning(
                    f'The following part of your input was truncated because CLIP can only handle sequences up to {self.tokenizer.model_max_length} tokens: {removed_text}'
                    )
                edit_concepts_input_ids = edit_concepts_input_ids[:, :self.
                    tokenizer.model_max_length]
            edit_concepts = self.text_encoder(edit_concepts_input_ids.to(
                self.device))[0]
        else:
            edit_concepts = editing_prompt_embeddings.to(self.device).repeat(
                batch_size, 1, 1)
        bs_embed_edit, seq_len_edit, _ = edit_concepts.shape
        edit_concepts = edit_concepts.repeat(1, num_images_per_prompt, 1)
        edit_concepts = edit_concepts.view(bs_embed_edit *
            num_images_per_prompt, seq_len_edit, -1)
    do_classifier_free_guidance = guidance_scale > 1.0
    if do_classifier_free_guidance:
        uncond_tokens: List[str]
        if negative_prompt is None:
            uncond_tokens = [''] * batch_size
        elif type(prompt) is not type(negative_prompt):
            raise TypeError(
                f'`negative_prompt` should be the same type to `prompt`, but got {type(negative_prompt)} != {type(prompt)}.'
                )
        elif isinstance(negative_prompt, str):
            uncond_tokens = [negative_prompt]
        elif batch_size != len(negative_prompt):
            raise ValueError(
                f'`negative_prompt`: {negative_prompt} has batch size {len(negative_prompt)}, but `prompt`: {prompt} has batch size {batch_size}. Please make sure that passed `negative_prompt` matches the batch size of `prompt`.'
                )
        else:
            uncond_tokens = negative_prompt
        max_length = text_input_ids.shape[-1]
        uncond_input = self.tokenizer(uncond_tokens, padding='max_length',
            max_length=max_length, truncation=True, return_tensors='pt')
        uncond_embeddings = self.text_encoder(uncond_input.input_ids.to(
            self.device))[0]
        seq_len = uncond_embeddings.shape[1]
        uncond_embeddings = uncond_embeddings.repeat(1,
            num_images_per_prompt, 1)
        uncond_embeddings = uncond_embeddings.view(batch_size *
            num_images_per_prompt, seq_len, -1)
        if enable_edit_guidance:
            text_embeddings = torch.cat([uncond_embeddings, text_embeddings,
                edit_concepts])
        else:
            text_embeddings = torch.cat([uncond_embeddings, text_embeddings])
    self.scheduler.set_timesteps(num_inference_steps, device=self.device)
    timesteps = self.scheduler.timesteps
    num_channels_latents = self.unet.config.in_channels
    latents = self.prepare_latents(batch_size * num_images_per_prompt,
        num_channels_latents, height, width, text_embeddings.dtype, self.
        device, generator, latents)
    extra_step_kwargs = self.prepare_extra_step_kwargs(generator, eta)
    edit_momentum = None
    self.uncond_estimates = None
    self.text_estimates = None
    self.edit_estimates = None
    self.sem_guidance = None
    for i, t in enumerate(self.progress_bar(timesteps)):
        latent_model_input = torch.cat([latents] * (2 +
            enabled_editing_prompts)
            ) if do_classifier_free_guidance else latents
        latent_model_input = self.scheduler.scale_model_input(
            latent_model_input, t)
        noise_pred = self.unet(latent_model_input, t, encoder_hidden_states
            =text_embeddings).sample
        if do_classifier_free_guidance:
            noise_pred_out = noise_pred.chunk(2 + enabled_editing_prompts)
            noise_pred_uncond, noise_pred_text = noise_pred_out[0
                ], noise_pred_out[1]
            noise_pred_edit_concepts = noise_pred_out[2:]
            noise_guidance = guidance_scale * (noise_pred_text -
                noise_pred_uncond)
            if self.uncond_estimates is None:
                self.uncond_estimates = torch.zeros((num_inference_steps + 
                    1, *noise_pred_uncond.shape))
            self.uncond_estimates[i] = noise_pred_uncond.detach().cpu()
            if self.text_estimates is None:
                self.text_estimates = torch.zeros((num_inference_steps + 1,
                    *noise_pred_text.shape))
            self.text_estimates[i] = noise_pred_text.detach().cpu()
            if self.edit_estimates is None and enable_edit_guidance:
                self.edit_estimates = torch.zeros((num_inference_steps + 1,
                    len(noise_pred_edit_concepts), *
                    noise_pred_edit_concepts[0].shape))
            if self.sem_guidance is None:
                self.sem_guidance = torch.zeros((num_inference_steps + 1, *
                    noise_pred_text.shape))
            if edit_momentum is None:
                edit_momentum = torch.zeros_like(noise_guidance)
            if enable_edit_guidance:
                concept_weights = torch.zeros((len(noise_pred_edit_concepts
                    ), noise_guidance.shape[0]), device=self.device, dtype=
                    noise_guidance.dtype)
                noise_guidance_edit = torch.zeros((len(
                    noise_pred_edit_concepts), *noise_guidance.shape),
                    device=self.device, dtype=noise_guidance.dtype)
                warmup_inds = []
                for c, noise_pred_edit_concept in enumerate(
                    noise_pred_edit_concepts):
                    self.edit_estimates[i, c] = noise_pred_edit_concept
                    if isinstance(edit_guidance_scale, list):
                        edit_guidance_scale_c = edit_guidance_scale[c]
                    else:
                        edit_guidance_scale_c = edit_guidance_scale
                    if isinstance(edit_threshold, list):
                        edit_threshold_c = edit_threshold[c]
                    else:
                        edit_threshold_c = edit_threshold
                    if isinstance(reverse_editing_direction, list):
                        reverse_editing_direction_c = (
                            reverse_editing_direction[c])
                    else:
                        reverse_editing_direction_c = reverse_editing_direction
                    if edit_weights:
                        edit_weight_c = edit_weights[c]
                    else:
                        edit_weight_c = 1.0
                    if isinstance(edit_warmup_steps, list):
                        edit_warmup_steps_c = edit_warmup_steps[c]
                    else:
                        edit_warmup_steps_c = edit_warmup_steps
                    if isinstance(edit_cooldown_steps, list):
                        edit_cooldown_steps_c = edit_cooldown_steps[c]
                    elif edit_cooldown_steps is None:
                        edit_cooldown_steps_c = i + 1
                    else:
                        edit_cooldown_steps_c = edit_cooldown_steps
                    if i >= edit_warmup_steps_c:
                        warmup_inds.append(c)
                    if i >= edit_cooldown_steps_c:
                        noise_guidance_edit[c, :, :, :, :] = torch.zeros_like(
                            noise_pred_edit_concept)
                        continue
                    noise_guidance_edit_tmp = (noise_pred_edit_concept -
                        noise_pred_uncond)
                    tmp_weights = (noise_guidance - noise_pred_edit_concept
                        ).sum(dim=(1, 2, 3))
                    tmp_weights = torch.full_like(tmp_weights, edit_weight_c)
                    if reverse_editing_direction_c:
                        noise_guidance_edit_tmp = noise_guidance_edit_tmp * -1
                    concept_weights[c, :] = tmp_weights
                    noise_guidance_edit_tmp = (noise_guidance_edit_tmp *
                        edit_guidance_scale_c)
                    if noise_guidance_edit_tmp.dtype == torch.float32:
                        tmp = torch.quantile(torch.abs(
                            noise_guidance_edit_tmp).flatten(start_dim=2),
                            edit_threshold_c, dim=2, keepdim=False)
                    else:
                        tmp = torch.quantile(torch.abs(
                            noise_guidance_edit_tmp).flatten(start_dim=2).
                            to(torch.float32), edit_threshold_c, dim=2,
                            keepdim=False).to(noise_guidance_edit_tmp.dtype)
                    noise_guidance_edit_tmp = torch.where(torch.abs(
                        noise_guidance_edit_tmp) >= tmp[:, :, None, None],
                        noise_guidance_edit_tmp, torch.zeros_like(
                        noise_guidance_edit_tmp))
                    noise_guidance_edit[c, :, :, :, :
                        ] = noise_guidance_edit_tmp
                warmup_inds = torch.tensor(warmup_inds).to(self.device)
                if len(noise_pred_edit_concepts) > warmup_inds.shape[0] > 0:
                    concept_weights = concept_weights.to('cpu')
                    noise_guidance_edit = noise_guidance_edit.to('cpu')
                    concept_weights_tmp = torch.index_select(concept_weights
                        .to(self.device), 0, warmup_inds)
                    concept_weights_tmp = torch.where(concept_weights_tmp <
                        0, torch.zeros_like(concept_weights_tmp),
                        concept_weights_tmp)
                    concept_weights_tmp = (concept_weights_tmp /
                        concept_weights_tmp.sum(dim=0))
                    noise_guidance_edit_tmp = torch.index_select(
                        noise_guidance_edit.to(self.device), 0, warmup_inds)
                    noise_guidance_edit_tmp = torch.einsum('cb,cbijk->bijk',
                        concept_weights_tmp, noise_guidance_edit_tmp)
                    noise_guidance_edit_tmp = noise_guidance_edit_tmp
                    noise_guidance = noise_guidance + noise_guidance_edit_tmp
                    self.sem_guidance[i] = noise_guidance_edit_tmp.detach(
                        ).cpu()
                    del noise_guidance_edit_tmp
                    del concept_weights_tmp
                    concept_weights = concept_weights.to(self.device)
                    noise_guidance_edit = noise_guidance_edit.to(self.device)
                concept_weights = torch.where(concept_weights < 0, torch.
                    zeros_like(concept_weights), concept_weights)
                concept_weights = torch.nan_to_num(concept_weights)
                noise_guidance_edit = torch.einsum('cb,cbijk->bijk',
                    concept_weights, noise_guidance_edit)
                noise_guidance_edit = (noise_guidance_edit + 
                    edit_momentum_scale * edit_momentum)
                edit_momentum = edit_mom_beta * edit_momentum + (1 -
                    edit_mom_beta) * noise_guidance_edit
                if warmup_inds.shape[0] == len(noise_pred_edit_concepts):
                    noise_guidance = noise_guidance + noise_guidance_edit
                    self.sem_guidance[i] = noise_guidance_edit.detach().cpu()
            if sem_guidance is not None:
                edit_guidance = sem_guidance[i].to(self.device)
                noise_guidance = noise_guidance + edit_guidance
            noise_pred = noise_pred_uncond + noise_guidance
        latents = self.scheduler.step(noise_pred, t, latents, **
            extra_step_kwargs).prev_sample
        if callback is not None and i % callback_steps == 0:
            step_idx = i // getattr(self.scheduler, 'order', 1)
            callback(step_idx, t, latents)
    if not output_type == 'latent':
        image = self.vae.decode(latents / self.vae.config.scaling_factor,
            return_dict=False)[0]
        image, has_nsfw_concept = self.run_safety_checker(image, self.
            device, text_embeddings.dtype)
    else:
        image = latents
        has_nsfw_concept = None
    if has_nsfw_concept is None:
        do_denormalize = [True] * image.shape[0]
    else:
        do_denormalize = [(not has_nsfw) for has_nsfw in has_nsfw_concept]
    image = self.image_processor.postprocess(image, output_type=output_type,
        do_denormalize=do_denormalize)
    if not return_dict:
        return image, has_nsfw_concept
    return SemanticStableDiffusionPipelineOutput(images=image,
        nsfw_content_detected=has_nsfw_concept)
