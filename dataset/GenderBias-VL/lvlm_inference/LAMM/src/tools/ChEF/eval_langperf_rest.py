import json
import os
import random
from tqdm import tqdm
import openai
import datetime
import time
import argparse
import re

openai.api_key=''
openai.organization = ""
openai.Model.list()
SYS_VQA="""
Please act as an impartial judge and conduct a comprehensive assessment of a multimodal AI assistant's performance in the field of Visual Question Answering (VQA). Each data sample to be evaluated follows the following format:
[Question]
{Question}\n
[Ground Truth]
{Ground truth}\n
[Assistant's Chain of Thought]
{Chain of thought generated by AI assistant}\n
[Assistant's Final Choice]
{Final Choice generated by AI assistant}\n
Your task is to evaluate the quality of natural language generation from AI assistant considering factors such as the helpfulness, relevance, accuracy, depth, creativity, and level of detail of the response. 
Please first provide a comprehensive explanation of your evaluation and an overall score ranging from 0 to 10 based on explanation, where a higher score indicates better overall performance. Please output in the following format:\n
[Explanation] {Evaluation Explanation}
[Overall Score] {An integer ranging from 0 to 10 representing the final evaluation score}\n
Please ensure that your evaluation score comprehensively captures the AI assistant's performance avoiding any potential bias. Assuming that the visual information mentioned by the AI assistant is contained in the image, you only need to evaluate the quality of the generated text. Your assessments will contribute to enhancing the assistant's effectiveness in visual question answering.
"""


DATA_Template = """
[Question]
{}\n
[Ground Truth]
{}\n
[Assistant's Chain of Thought]
{}\n
[Assistant's Final Choice]
{}\n
"""







    



if __name__ == '__main__':
    parser = argparse.ArgumentParser()
    parser.add_argument('--base-data-path', default='../data/LAMM/2D_Benchmark/meta_file/VQA_ScienceQA.json')
    parser.add_argument('--answer-path', type=str, required=True)
    parser.add_argument('--response-dir', default='../results/gpt_eval')
    args = parser.parse_args()
    response_list, res_list = GPT_Metric(args.base_data_path,
                               args.answer_path,
                             args.response_dir)