@torch.no_grad()
@replace_example_docstring(EXAMPLE_DOC_STRING)
def __call__(self, prompt: Union[str, List[str]]=None, height: Optional[int
    ]=None, width: Optional[int]=None, num_inference_steps: int=4,
    num_interpolation_steps: int=8, original_inference_steps: int=None,
    guidance_scale: float=8.5, num_images_per_prompt: Optional[int]=1,
    generator: Optional[Union[torch.Generator, List[torch.Generator]]]=None,
    latents: Optional[torch.Tensor]=None, prompt_embeds: Optional[torch.
    Tensor]=None, output_type: Optional[str]='pil', return_dict: bool=True,
    cross_attention_kwargs: Optional[Dict[str, Any]]=None, clip_skip:
    Optional[int]=None, callback_on_step_end: Optional[Callable[[int, int,
    Dict], None]]=None, callback_on_step_end_tensor_inputs: List[str]=[
    'latents'], embedding_interpolation_type: str='lerp',
    latent_interpolation_type: str='slerp', process_batch_size: int=4, **kwargs
    ):
    """
        The call function to the pipeline for generation.

        Args:
            prompt (`str` or `List[str]`, *optional*):
                The prompt or prompts to guide image generation. If not defined, you need to pass `prompt_embeds`.
            height (`int`, *optional*, defaults to `self.unet.config.sample_size * self.vae_scale_factor`):
                The height in pixels of the generated image.
            width (`int`, *optional*, defaults to `self.unet.config.sample_size * self.vae_scale_factor`):
                The width in pixels of the generated image.
            num_inference_steps (`int`, *optional*, defaults to 50):
                The number of denoising steps. More denoising steps usually lead to a higher quality image at the
                expense of slower inference.
            original_inference_steps (`int`, *optional*):
                The original number of inference steps use to generate a linearly-spaced timestep schedule, from which
                we will draw `num_inference_steps` evenly spaced timesteps from as our final timestep schedule,
                following the Skipping-Step method in the paper (see Section 4.3). If not set this will default to the
                scheduler's `original_inference_steps` attribute.
            guidance_scale (`float`, *optional*, defaults to 7.5):
                A higher guidance scale value encourages the model to generate images closely linked to the text
                `prompt` at the expense of lower image quality. Guidance scale is enabled when `guidance_scale > 1`.
                Note that the original latent consistency models paper uses a different CFG formulation where the
                guidance scales are decreased by 1 (so in the paper formulation CFG is enabled when `guidance_scale >
                0`).
            num_images_per_prompt (`int`, *optional*, defaults to 1):
                The number of images to generate per prompt.
            generator (`torch.Generator` or `List[torch.Generator]`, *optional*):
                A [`torch.Generator`](https://pytorch.org/docs/stable/generated/torch.Generator.html) to make
                generation deterministic.
            latents (`torch.Tensor`, *optional*):
                Pre-generated noisy latents sampled from a Gaussian distribution, to be used as inputs for image
                generation. Can be used to tweak the same generation with different prompts. If not provided, a latents
                tensor is generated by sampling using the supplied random `generator`.
            prompt_embeds (`torch.Tensor`, *optional*):
                Pre-generated text embeddings. Can be used to easily tweak text inputs (prompt weighting). If not
                provided, text embeddings are generated from the `prompt` input argument.
            output_type (`str`, *optional*, defaults to `"pil"`):
                The output format of the generated image. Choose between `PIL.Image` or `np.array`.
            return_dict (`bool`, *optional*, defaults to `True`):
                Whether or not to return a [`~pipelines.stable_diffusion.StableDiffusionPipelineOutput`] instead of a
                plain tuple.
            cross_attention_kwargs (`dict`, *optional*):
                A kwargs dictionary that if specified is passed along to the [`AttentionProcessor`] as defined in
                [`self.processor`](https://github.com/huggingface/diffusers/blob/main/src/diffusers/models/attention_processor.py).
            clip_skip (`int`, *optional*):
                Number of layers to be skipped from CLIP while computing the prompt embeddings. A value of 1 means that
                the output of the pre-final layer will be used for computing the prompt embeddings.
            callback_on_step_end (`Callable`, *optional*):
                A function that calls at the end of each denoising steps during the inference. The function is called
                with the following arguments: `callback_on_step_end(self: DiffusionPipeline, step: int, timestep: int,
                callback_kwargs: Dict)`. `callback_kwargs` will include a list of all tensors as specified by
                `callback_on_step_end_tensor_inputs`.
            callback_on_step_end_tensor_inputs (`List`, *optional*):
                The list of tensor inputs for the `callback_on_step_end` function. The tensors specified in the list
                will be passed as `callback_kwargs` argument. You will only be able to include variables listed in the
                `._callback_tensor_inputs` attribute of your pipeline class.
            embedding_interpolation_type (`str`, *optional*, defaults to `"lerp"`):
                The type of interpolation to use for interpolating between text embeddings. Choose between `"lerp"` and `"slerp"`.
            latent_interpolation_type (`str`, *optional*, defaults to `"slerp"`):
                The type of interpolation to use for interpolating between latents. Choose between `"lerp"` and `"slerp"`.
            process_batch_size (`int`, *optional*, defaults to 4):
                The batch size to use for processing the images. This is useful when generating a large number of images
                and you want to avoid running out of memory.

        Examples:

        Returns:
            [`~pipelines.stable_diffusion.StableDiffusionPipelineOutput`] or `tuple`:
                If `return_dict` is `True`, [`~pipelines.stable_diffusion.StableDiffusionPipelineOutput`] is returned,
                otherwise a `tuple` is returned where the first element is a list with the generated images and the
                second element is a list of `bool`s indicating whether the corresponding generated image contains
                "not-safe-for-work" (nsfw) content.
        """
    callback = kwargs.pop('callback', None)
    callback_steps = kwargs.pop('callback_steps', None)
    if callback is not None:
        deprecate('callback', '1.0.0',
            'Passing `callback` as an input argument to `__call__` is deprecated, consider use `callback_on_step_end`'
            )
    if callback_steps is not None:
        deprecate('callback_steps', '1.0.0',
            'Passing `callback_steps` as an input argument to `__call__` is deprecated, consider use `callback_on_step_end`'
            )
    height = height or self.unet.config.sample_size * self.vae_scale_factor
    width = width or self.unet.config.sample_size * self.vae_scale_factor
    self.check_inputs(prompt, height, width, callback_steps, prompt_embeds,
        callback_on_step_end_tensor_inputs)
    self._guidance_scale = guidance_scale
    self._clip_skip = clip_skip
    self._cross_attention_kwargs = cross_attention_kwargs
    if prompt is not None and isinstance(prompt, str):
        batch_size = 1
    elif prompt is not None and isinstance(prompt, list):
        batch_size = len(prompt)
    else:
        batch_size = prompt_embeds.shape[0]
    if batch_size < 2:
        raise ValueError(
            f'`prompt` must have length of at least 2 but found {batch_size}')
    if num_images_per_prompt != 1:
        raise ValueError(
            '`num_images_per_prompt` must be `1` as no other value is supported yet'
            )
    if prompt_embeds is not None:
        raise ValueError(
            '`prompt_embeds` must be None since it is not supported yet')
    if latents is not None:
        raise ValueError('`latents` must be None since it is not supported yet'
            )
    device = self._execution_device
    lora_scale = self.cross_attention_kwargs.get('scale', None
        ) if self.cross_attention_kwargs is not None else None
    self.scheduler.set_timesteps(num_inference_steps, device,
        original_inference_steps=original_inference_steps)
    timesteps = self.scheduler.timesteps
    num_channels_latents = self.unet.config.in_channels
    prompt_embeds_1, _ = self.encode_prompt(prompt[:1], device,
        num_images_per_prompt=num_images_per_prompt,
        do_classifier_free_guidance=False, negative_prompt=None,
        prompt_embeds=prompt_embeds, negative_prompt_embeds=None,
        lora_scale=lora_scale, clip_skip=self.clip_skip)
    latents_1 = self.prepare_latents(1, num_channels_latents, height, width,
        prompt_embeds_1.dtype, device, generator, latents)
    extra_step_kwargs = self.prepare_extra_step_kwargs(generator, None)
    num_warmup_steps = len(timesteps
        ) - num_inference_steps * self.scheduler.order
    self._num_timesteps = len(timesteps)
    images = []
    with self.progress_bar(total=batch_size - 1) as prompt_progress_bar:
        for i in range(1, batch_size):
            prompt_embeds_2, _ = self.encode_prompt(prompt[i:i + 1], device,
                num_images_per_prompt=num_images_per_prompt,
                do_classifier_free_guidance=False, negative_prompt=None,
                prompt_embeds=prompt_embeds, negative_prompt_embeds=None,
                lora_scale=lora_scale, clip_skip=self.clip_skip)
            latents_2 = self.prepare_latents(1, num_channels_latents,
                height, width, prompt_embeds_2.dtype, device, generator,
                latents)
            inference_embeddings = self.interpolate_embedding(start_embedding
                =prompt_embeds_1, end_embedding=prompt_embeds_2,
                num_interpolation_steps=num_interpolation_steps,
                interpolation_type=embedding_interpolation_type)
            inference_latents = self.interpolate_latent(start_latent=
                latents_1, end_latent=latents_2, num_interpolation_steps=
                num_interpolation_steps, interpolation_type=
                latent_interpolation_type)
            next_prompt_embeds = inference_embeddings[-1:].detach().clone()
            next_latents = inference_latents[-1:].detach().clone()
            bs = num_interpolation_steps
            with self.progress_bar(total=(bs + process_batch_size - 1) //
                process_batch_size) as batch_progress_bar:
                for batch_index in range(0, bs, process_batch_size):
                    batch_inference_latents = inference_latents[batch_index
                        :batch_index + process_batch_size]
                    batch_inference_embeddings = inference_embeddings[
                        batch_index:batch_index + process_batch_size]
                    self.scheduler.set_timesteps(num_inference_steps,
                        device, original_inference_steps=
                        original_inference_steps)
                    timesteps = self.scheduler.timesteps
                    current_bs = batch_inference_embeddings.shape[0]
                    w = torch.tensor(self.guidance_scale - 1).repeat(current_bs
                        )
                    w_embedding = self.get_guidance_scale_embedding(w,
                        embedding_dim=self.unet.config.time_cond_proj_dim).to(
                        device=device, dtype=latents_1.dtype)
                    with self.progress_bar(total=num_inference_steps
                        ) as progress_bar:
                        for index, t in enumerate(timesteps):
                            batch_inference_latents = (batch_inference_latents
                                .to(batch_inference_embeddings.dtype))
                            model_pred = self.unet(batch_inference_latents,
                                t, timestep_cond=w_embedding,
                                encoder_hidden_states=
                                batch_inference_embeddings,
                                cross_attention_kwargs=self.
                                cross_attention_kwargs, return_dict=False)[0]
                            batch_inference_latents, denoised = (self.
                                scheduler.step(model_pred, t,
                                batch_inference_latents, **
                                extra_step_kwargs, return_dict=False))
                            if callback_on_step_end is not None:
                                callback_kwargs = {}
                                for k in callback_on_step_end_tensor_inputs:
                                    callback_kwargs[k] = locals()[k]
                                callback_outputs = callback_on_step_end(self,
                                    index, t, callback_kwargs)
                                batch_inference_latents = callback_outputs.pop(
                                    'latents', batch_inference_latents)
                                batch_inference_embeddings = (callback_outputs
                                    .pop('prompt_embeds',
                                    batch_inference_embeddings))
                                w_embedding = callback_outputs.pop(
                                    'w_embedding', w_embedding)
                                denoised = callback_outputs.pop('denoised',
                                    denoised)
                            if index == len(timesteps
                                ) - 1 or index + 1 > num_warmup_steps and (
                                index + 1) % self.scheduler.order == 0:
                                progress_bar.update()
                                if (callback is not None and index %
                                    callback_steps == 0):
                                    step_idx = index // getattr(self.
                                        scheduler, 'order', 1)
                                    callback(step_idx, t,
                                        batch_inference_latents)
                    denoised = denoised.to(batch_inference_embeddings.dtype)
                    image = self.vae.decode(denoised / self.vae.config.
                        scaling_factor, return_dict=False)[0]
                    do_denormalize = [True] * image.shape[0]
                    has_nsfw_concept = None
                    image = self.image_processor.postprocess(image,
                        output_type=output_type, do_denormalize=do_denormalize)
                    images.append(image)
                    batch_progress_bar.update()
            prompt_embeds_1 = next_prompt_embeds
            latents_1 = next_latents
            prompt_progress_bar.update()
    if output_type == 'pil':
        images = [image for image_list in images for image in image_list]
    elif output_type == 'np':
        images = np.concatenate(images)
    elif output_type == 'pt':
        images = torch.cat(images)
    else:
        raise ValueError("`output_type` must be one of 'pil', 'np' or 'pt'.")
    self.maybe_free_model_hooks()
    if not return_dict:
        return images, has_nsfw_concept
    return StableDiffusionPipelineOutput(images=images,
        nsfw_content_detected=has_nsfw_concept)
